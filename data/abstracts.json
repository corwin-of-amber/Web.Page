{
  "OOPSLA2016": "<p>We introduce a framework allowing domain experts to manipulate computational terms in the interest of deriving better, more efficient implementations. It employs deductive reasoning to generate provably correct efficient implementations from a very high-level specification of an algorithm, and inductive constraint-based synthesis to improve automation. Semantic information is encoded into program terms through the use of refinement types. In this paper, we develop the technique in the context of a system called Bellmania that uses solver-aided tactics to derive parallel divide-and-conquer implementations of dynamic programming algorithms that have better locality and are significantly more efficient than traditional loop-based implementations. Bellmania includes a high-level language for specifying dynamic programming algorithms and a calculus that facilitates gradual transformation of these specifications into efficient implementations. These transformations formalize the divide-and-conquer technique; a visualization interface helps users to interactively guide the process, while an SMT-based back-end verifies each step and takes care of low-level reasoning required for parallelism. We have used the system to generate provably correct implementations of several algorithms, including some important algorithms from computational biology, and show that the performance is comparable to that of the best manually optimized code.",
  "ESOP2021": "<p>Determining upper bounds on the time complexity of a program is a fundamental problem with a variety of applications, such as performance debugging, resource certification, and compile-time optimizations.\nAutomated techniques for cost analysis excel at\nbounding the resource complexity of programs that use integer values and linear arithmetic.\nUnfortunately, they fall short when execution traces become more involved, esp. when data dependencies may affect the termination conditions of loops.\nIn such cases, state-of-the-art analyzers have shown to produce loose bounds, or even no bound at all.</p><p>We propose a novel technique  that generalizes the common notion of recurrence relations based on ranking functions.\nExisting methods usually unfold one loop iteration, and examine\nthe resulting relations between variables.\nThese relations assist in establishing a recurrence that bounds the number of loop iterations.\nWe propose a different approach, where we derive recurrences by comparing <span class=\"emph\">whole traces</span> with <span class=\"emph\">whole traces</span> of a lower rank,\navoiding the need to analyze the complexity of intermediate states.\nWe offer a set of global properties, defined with respect to whole traces, that facilitate such a comparison,\nand show that these properties can be checked efficiently using a handful of local conditions.\nTo this end, we adapt <span class=\"emph\">state squeezers</span>, an induction mechanism previously used for verifying safety properties.\nWe demonstrate that this technique encompasses the reasoning power of bounded unfolding, and more.\nWe present some seemingly innocuous, yet intricate, examples where previous tools based on <span class=\"emph\">cost relations</span> and control flow analysis fail to solve,\nand that our squeezer-powered approach succeeds.</p>",
  "ICFP2021": "<p>We present Lifty, a domain-specific language for data-centric applications that manipulate sensitive data. \nA Lifty programmer annotates the sources of sensitive data with declarative security policies,\nand the language statically and automatically verifies that the application handles the data according to the policies.\nMoreover, if verification fails, Lifty suggests a provably correct repair,\nthereby easing the programmer burden of implementing policy enforcing code throughout the application.</p><p>The main insight behind Lifty is to encode information flow control\nusing <span class=\"emph\">liquid types</span>, an expressive yet decidable type system.\nLiquid types enable fully automatic checking of complex, data dependent policies,\nand power our repair mechanism via type-driven error localization and patch synthesis.\nOur experience using Lifty to implement three case studies from the literature shows that \n\n<span class=\"item\">◦</span> the Lifty policy language is sufficiently expressive to specify many real-world policies,\n<span class=\"item\">◦</span> the Lifty type checker is able to verify secure programs and find leaks in insecure programs quickly, and \n<span class=\"item\">◦</span> even if the programmer leaves out <span class=\"emph\">all</span> policy enforcing code, \nthe Lifty repair engine is able to patch all leaks automatically within a reasonable time. \n</p><p></p>",
  "ESOP2024": "<p>Hyperproperties govern the behavior of a system or systems across multiple executions, and are being recognized as an important extension of regular temporal properties. So far, such properties have resisted comprehensive treatment by modern software model-checking approaches such as IC3/PDR, due to the need to find not only an inductive invariant but also a <i>total</i> alignment of different executions that facilitates simpler inductive invariants. We show how this treatment is achieved via a reduction from the verification problem of <span style=\"font-size: 80%\">∀<sup><i>k</i></sup>∃<sup><i>l</i></sup></span> properties to Constrained Horn Clauses. The approach is based on combining the inference of an alignment and inductive invariant in a single CHC encoding; and, for existential quantification over traces, incorporating also inference of a witness function for the existential choices, based on a game semantics with a sound-and-complete encoding to CHCs as well.</p>"
}